---
layout: post
title: "What is Prompt Engineering?"
subtitle: "이건어디에들어가는거지"
category: PE
tags: 프롬프트엔지니어 PE
comments: true
---

# About Prompt Engineering : 6. 생각의 사슬 프롬프트

앞서 few shot여러개를 넣어봤는데도 불구하고 원하는 답을 얻지 못했습니다. 모델이 학습한 내용만으로는 원할한 작업을 수행하지 못하였는데요, Chain-of-Thought Prompting(생각의 사슬) 프롬프트를 통해서 문제를 해결할 수 있었습니다.

작년 구글에서 게재한 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'라는 논문을 시작으로 LLM의 성능이 크게 향상되었다고 하는데요, 이것이 바로 Chain-of-Thought Prompting(생각의 사슬)입니다. [(논문 링크)](https://arxiv.org/pdf/2201.11903.pdf)

CoT는 문제의 인과 관계에 대해서 차근차근 풀어서 전개하다 보면, 정답에 더 잘 도달할 수 있다는 개념입니다. 프롬프트를 구성할 때 '문제, 답' 만 줬다면, 이제는 '문제, 풀이, 답'을 주는 것입니다. 아래와 같이 말이죠.

![Untitled](/assets/img/About%20Prompt%20Engineering/cot/Untitled.png)
*이미지 소스: [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)*



위 논문에서 소개된 생각의 사슬(Chain-of-Thought) 프롬프트는 중간 추론 단계를 통해 복잡한 추론 기능을 가능하게 합니다. 추론이 필요한 복잡한 작업에 대해 더 나은 결과를 얻기 위해 퓨 샷 프롬프트와 결합할 수 있습니다.

지난번에 퓨 샷 프롬프트에서 다뤘던 문제를 다시 보겠습니다. 

![Untitled](/assets/img/About%20Prompt%20Engineering/cot/Untitled%201.png)

CoT를 활용해 중간 추론 단계를 예시로 들고 더 나은 결과가 나오는지 확인해보겠습니다.

![Untitled](/assets/img/About%20Prompt%20Engineering/cot/Untitled%202.png)

정답을 맞췄습니다. 간단한 풀이과정을 보여줬을 뿐인데 못 풀던 문제를 바로 풀어벼렸습니다.

신기하지 않나요?

![Untitled](/assets/img/About%20Prompt%20Engineering/cot/Untitled%203.png)

그리고 예시도 4개가 아닌 단 하나로도 충분한 결과를 출력할 수 있었습니다. 

### 제로 샷 CoT 프롬프트

그러면 이 생각의 사슬 프롬프트는 퓨 샷으로 사용할 수 밖에 없을까요? 일일이 예제를 수작업으로 만들어야 하고, 예제를 적용하기에는 너무 길거나 애매한 지시가 들어갈 수 있습니다. 이런 번거로움을 해결하기 위한 최근에 나온 아이디어 중 하나는 [제로샷 CoT(Kojima 2022)](https://arxiv.org/abs/2205.11916)라는 아이디어로, 프롬프트에 "let’s think step by step"를 추가하는 것입니다. 기본적으로 포함됩니다. 간단한 문제를 시도하여 모델의 성능을 확인해 보겠습니다.

![Untitled](/assets/img/About%20Prompt%20Engineering/cot/Untitled%204.png)

간단한 문제라고 했지만 사실 아직 gpt3.5에게는 간단하지 않았습니다. 산술에 있어서는 아직 버거운 모습입니다. 

앞서 제시한 CoT를 활용해 모델의 성능을 다시한번 확인해보겠습니다.

"let’s think step by step"에 대한 번역으로 무엇이 좋을까요? 정확도가 조금 떨어지긴 하지만, 한국어를 사용해보겠습니다. 

![Untitled](/assets/img/About%20Prompt%20Engineering/cot/Untitled%205.png)

‘단계별로 생각해봅시다’ ‘한 단계, 한 단계 생각하고 답해주세요’ 등등 여러번의 시행착오 끝에 원하는 답을 이끌어 낼 수 있었는데요, 간단한 한 문장을 추가해서 답을 이끌어낸 것이 매우 인상적입니다. 프롬프트에 사용할 예제가 마땅치 않은 경우, 특히 유용하게 사용할 수 있습니다.

### Automatic Chain-of-Thought

앞서 보신 것 처럼, CoT 프롬프트는 크게 두 가지 패러다임으로 분류할 수 있었는데요, 하나는  “Let’s think step by step”프롬프트를 구성해 주는것, 그리고 다른 하나는  추론 예제를 직접 생성하는 것입니다. “Let’s think step by step”를 통해서 우리가 수동적으로 만들었던 예시가 없어도 된다는 것을 보여줬지만, 이렇게 생성된 체인들은 비교적 낮은 정확도를 보여줍니다.

이러한 문제를 완화 하기 위해서 [Zhang et al. (2022)](https://arxiv.org/abs/2210.03493)는 Automatic Chain-of-Thought을 제안했습니다. 

Auto-CoT는 다음 두 가지 주요 단계로 구성됩니다:

1단계: 질문 클러스터링: 주어진 데이터 세트의 질문을 몇 개의 클러스터로 분할

2단계: 시연 샘플링: 각 클러스터에서 대표 질문을 선택하고 간단한 휴리스틱이 포함된 Zero-Shot-CoT를 사용하여 추론 체인을 생성

프로세스는 아래와 같이 구성됩니다.

![Untitled](/assets/img/About%20Prompt%20Engineering/cot/Untitled%206.png)

---

## reference

- [https://mpost.io/prompt-engineering-ultimate-guide/](https://mpost.io/prompt-engineering-ultimate-guide/)
- [https://www.promptingguide.ai/](https://www.promptingguide.ai/)
- [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
- [https://github.com/amazon-science/auto-cot](https://github.com/amazon-science/auto-cot)